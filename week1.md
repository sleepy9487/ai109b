* 人工智慧的領域著重於設備  
* 一般的電腦在人工智慧領域上非常龜速  
* 以一般學校或者學生而言 訓練模型是不可能的事  
* 但可以去網路上租用專用的CPU 但非常燒錢  
* 課程重點後期的深度學習很重要  
* 但因為算力不夠很難去做實行 但可以運用別人已經訓練好的模型  
* 近期因深度學習的熱度上升所以人工智慧領域逐漸熱門  

* 人工智慧分成兩種  
* 傳統模式(搜尋+優化)跟機器學習  
* 而機器學習分成統計與神經網路  
* 而近年來神經網路因深度學習所應用的層次非常多而被重視  
* 許多訓練出的模型也被證實出來是有效的  
* 比如CNN,RNN,LSTM,BERT  

* 傳統模式的搜尋方式不同於過去課程上的廣域搜尋法等  
* 是解空間搜尋法 比如下棋的Min-Max  

# 應用  
* 影像  
  * 傳統類 : 先抽取特徵再做辨識  
* 深度學習(CNN/Yolo) : 不抽特徵,硬做,全部透過梯度下降法,找到錯誤率最低的模型  
>Yolo的原作者在第三代就不做了,因為有隱憂,比如被利用於大規模監控  

# 語言  
* 規則  
* 統計  
* RNN/BERT  
* 近年來自動翻譯的功能有所提升,但有所加強,但所有公司面臨了一種問題-落地  
* 落地 - 能夠在工業上展示,並且有商業價值  
  * 落地失敗例子 : 有道翻譯雖說翻譯做得很好 但實際上人們很少會實際使用帶有其的手機去跟外國人溝通(使用者體驗問題)  
  * 落地成功例子 : 機器狗做監工或者探雷等等  
* 翻譯理想中的例子 : 電影中雙方透過耳麥直接轉換語言  

# 下棋  
* Min-Max  
![Min-Max](https://github.com/sleepy9487/ai109b/blob/main/ai-image/Min-Max.jpg)
* MCTS  
# 自動控制  
* 機器人/車  

# 優化  
* 數學 ---> 科學計算  
* 代數  
* 幾何  
* 微積分 - 偏微分(梯度下降法) 鏈鎖規則讓梯度下降法->evolve成反傳遞演算法(現今深度學習最重要的演算法)  
  機率統計  

>重要程度 微積分>機率統計  

# 爬山演算法  
* 爬山演算法是一種簡單的優化算法,不斷尋找最低點,當找到最低點回傳結果  
  當我們要找的是最高點，可以將取線x(-1) 這樣就像是人類在爬山一樣  

* 簡單理論 :  
  先看左再看右->看哪邊高有往哪走  
  當如果左右都一樣高 -> 頂點=答案  

>可能遇到的問題 : 如果遇到很多個山峰或山谷,只靠看左右會無法找到最佳解  


## 實驗1  

# 簡易爬山演算法 -- 針對單變數函數  
```
def hillClimbing(f, x, dx=0.01):  
    while (True):  
        print('x={0:.5f} f(x)={1:.5f}'.format(x, f(x)))  
        if f(x+dx)>f(x): # 如果右邊的高度 f(x+dx) > 目前高度 f(x) ，那麼就往右走  
            x = x + dx  
        elif f(x-dx)>f(x): # 如果左邊的高度 f(x-dx) > 目前高度 f(x) ，那麼就往左走  
            x = x - dx  
        else: # 如果兩邊都沒有比現在的 f(x) 高，那麼這裡就是區域最高點，直接中斷傳回  
            break  
    return x  

# 高度函數  
def f(x): #x只能是單一數值,不能是向量  
    return -1*(x*x-2*x+1)  
    # return -1*(x*x+3*x+5)  
    # return -1*abs(x*x-4)  

hillClimbing(f, 0) # 以 x=0 為起點，開始呼叫爬山演算法  
```

# result  
```
x=0.00000 f(x)=-1.00000  
x=0.01000 f(x)=-0.98010  
x=0.02000 f(x)=-0.96040  
x=0.03000 f(x)=-0.94090  
x=0.04000 f(x)=-0.92160  
x=0.05000 f(x)=-0.90250  
x=0.06000 f(x)=-0.88360  
x=0.07000 f(x)=-0.86490  

.  
.  
.  

x=0.88000 f(x)=-0.01440  
x=0.89000 f(x)=-0.01210  
x=0.90000 f(x)=-0.01000  
x=0.91000 f(x)=-0.00810  
x=0.92000 f(x)=-0.00640  
x=0.93000 f(x)=-0.00490  
x=0.94000 f(x)=-0.00360  
x=0.95000 f(x)=-0.00250  
x=0.96000 f(x)=-0.00160  
x=0.97000 f(x)=-0.00090  
x=0.98000 f(x)=-0.00040  
x=0.99000 f(x)=-0.00010  
x=1.00000 f(x)=-0.00000  
```

# 實驗2
```
import random  

def hillClimbing(f, x, y, h=0.01):  
    failCount = 0                    # 失敗次數歸零  
    while (failCount < 10000):       # 如果失敗次數小於一萬次就繼續執行  
        fxy = f(x, y)                # fxy 為目前高度  
        dx = random.uniform(-h, h)   # dx 為左右偏移量  
        dy = random.uniform(-h, h)   # dy 為前後偏移量  
        if f(x+dx, y+dy) >= fxy:     # 如果移動後高度比現在高  
            x = x + dx               #   就移過去  
            y = y + dy
            print('x={:.3f} y={:.3f} f(x,y)={:.3f}'.format(x, y, fxy))  
            failCount = 0            # 失敗次數歸零  
        else:                        # 若沒有更高  
            failCount = failCount + 1#   那就又失敗一次  
    return (x,y,fxy)                 # 結束傳回 （已經失敗超過一萬次了）   

def f(x, y):  
    return -1 * ( x*x -2*x + y*y +2*y - 8 )  

hillClimbing(f, 0, 0)  
```
>比起上面的爬山演算法,選擇偏移量  
```
result  

x=-0.001 y=-0.007 f(x,y)=8.000  
x=-0.002 y=-0.014 f(x,y)=8.012  
x=0.004 y=-0.013 f(x,y)=8.024  
x=0.012 y=-0.016 f(x,y)=8.035  
x=0.015 y=-0.024 f(x,y)=8.056  
x=0.022 y=-0.033 f(x,y)=8.077  
x=0.028 y=-0.032 f(x,y)=8.108 
x=0.036 y=-0.034 f(x,y)=8.118 
.  
.  
.  
x=0.999 y=-0.997 f(x,y)=10.000  
x=0.998 y=-1.000 f(x,y)=10.000  
x=1.001 y=-0.999 f(x,y)=10.000  
x=1.000 y=-0.999 f(x,y)=10.000  
x=1.000 y=-1.000 f(x,y)=10.000  
```

